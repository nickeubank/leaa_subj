{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hKPY4yZo9neD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "os.environ[\"PATH\"] = (\n",
        "    \"/opt/apps/rhel9/cuda-12.4/bin:/opt/apps/rhel9/cuda-12.4/\" + os.environ[\"PATH\"]\n",
        ")\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = (\n",
        "    \"/opt/apps/rhel9/cuda-12.4/bin:/opt/apps/rhel9/cuda-12.4\"\n",
        "    + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        ")\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (  # AdamW,\n",
        "    BertForSequenceClassification,\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "pd.set_option(\"mode.copy_on_write\", True)\n",
        "repo_id = \"nickeubank/leaa_grant_subjects\"\n",
        "workingdir = \"/hpc/group/ssri/nce8/leaa_subj/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqAoIdGErrRL",
        "outputId": "da24dad9-3707-4a55-b04c-c567447deea1"
      },
      "outputs": [],
      "source": [
        "# dir = \"https://github.com/nickeubank/leaa_subj/raw/refs/heads/main/\"\n",
        "grants = pd.read_parquet(workingdir + \"subj_text_and_labels.parquet\")\n",
        "\n",
        "#########\n",
        "# Split into train test and for predict\n",
        "#########\n",
        "grants = grants.drop_duplicates(\"description\")\n",
        "\n",
        "labeled = grants[grants[\"label_1\"].notnull()]\n",
        "labeled[\"label_1_encoded\"] = labeled[\"label_1\"] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64148\n",
            "16038\n"
          ]
        }
      ],
      "source": [
        "labeled = labeled.sort_values(\"description\")\n",
        "\n",
        "train_label, test_label, train_text, test_text = train_test_split(\n",
        "    labeled[\"label_1_encoded\"].values,\n",
        "    labeled[\"description\"].values,\n",
        "    test_size=0.2,\n",
        "    random_state=45,\n",
        "    stratify=labeled[\"label_1\"],\n",
        ")\n",
        "print(len(train_label))\n",
        "print(len(test_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Hpg0YddBshl9"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# Preprocess\n",
        "########\n",
        "\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hypertuning Parameters\n",
        "# hypers = {\"lr\": [], \"mlen\": [], \"batch_size\": [], \"accuracy\": []}\n",
        "# hypers = pd.read_parquet(\"hyperparams_1digit_bigbert.parquet\").to_dict()\n",
        "# for k in hypers.keys():\n",
        "#     hypers[k] = list(hypers[k].values())\n",
        "\n",
        "# df = pd.read_parquet(\"hyperparams_1digit_bigbert.parquet\")\n",
        "# df.sort_values(\"accuracy\", ascending=False)\n",
        "\n",
        "# params = [\n",
        "#     {\"mlen\": mlen, \"batch_size\": batch_size, \"lr\": lr}\n",
        "#     for batch_size in [16, 32]\n",
        "#     for mlen in [128, 256, 512]\n",
        "#     for lr in [1e-7, 1e-6, 1e-5, 1e-4]\n",
        "# ]\n",
        "# params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 1003/1003 [11:49<00:00,  1.41it/s, loss=1.39]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.4512595287408097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 1003/1003 [11:49<00:00,  1.41it/s, loss=1.53]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.4740087474925687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 1003/1003 [11:49<00:00,  1.41it/s, loss=1.28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.4759330496949665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 1003/1003 [11:49<00:00,  1.41it/s, loss=1.48]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 1.4737451928681653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 1003/1003 [11:49<00:00,  1.41it/s, loss=1.67]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 1.4760060884422461\n",
            "Validation Accuracy: 0.4120\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [03:05<00:00,  2.71it/s, loss=1.29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.584797185730649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [03:05<00:00,  2.71it/s, loss=1.8] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.4694474814422578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [03:03<00:00,  2.73it/s, loss=1.49]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.4187838698763295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [03:03<00:00,  2.73it/s, loss=1.65]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 1.3611481560178962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [03:03<00:00,  2.73it/s, loss=1.32] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 1.2689757027711526\n",
            "Validation Accuracy: 0.5851\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [03:06<00:00,  2.69it/s, loss=0.916]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.2199355082445411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [03:05<00:00,  2.70it/s, loss=0.962]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.7081139040183261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [03:08<00:00,  2.66it/s, loss=0.139]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.5491387742270987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [03:06<00:00,  2.69it/s, loss=1.33] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 0.4761932602975948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [03:06<00:00,  2.69it/s, loss=0.278]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 0.4338548208969048\n",
            "Validation Accuracy: 0.8651\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [03:06<00:00,  2.69it/s, loss=0.929] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 0.5770211536214647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [03:08<00:00,  2.67it/s, loss=0.108] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.33906245411214125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [03:06<00:00,  2.69it/s, loss=0.0604]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.2689175803689249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [03:05<00:00,  2.71it/s, loss=0.0585]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 0.19729286674187477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [03:06<00:00,  2.70it/s, loss=0.0152]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 0.1512523057881698\n",
            "Validation Accuracy: 0.8742\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [03:04<00:00,  2.72it/s, loss=1.4]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.0994551437665743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [03:05<00:00,  2.71it/s, loss=1.34]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.4746730054517192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [03:03<00:00,  2.73it/s, loss=1.18]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.4722514777069549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [03:05<00:00,  2.71it/s, loss=1.6] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 1.469263973464054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [03:05<00:00,  2.70it/s, loss=1.44]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 1.4726735626558858\n",
            "Validation Accuracy: 0.4120\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=1.59]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.5676026759869548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=1.38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.457659854356986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=1.21]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.3493546967012473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [05:38<00:00,  1.48it/s, loss=1.05] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 1.2456859879759679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=0.887]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 1.1657552784420104\n",
            "Validation Accuracy: 0.6451\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128, 256], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688, 0.6451331296377127]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [05:38<00:00,  1.48it/s, loss=1.08] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.2904329822357907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=0.132]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.6628474878718653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [05:38<00:00,  1.48it/s, loss=0.144]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.5124314470713832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=2.97] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 0.4534685044323068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [05:38<00:00,  1.48it/s, loss=0.623]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 0.41530938010173013\n",
            "Validation Accuracy: 0.8695\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128, 256, 256], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688, 0.6451331296377127, 0.8694893059799215]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=0.0514]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 0.6044451224272588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [05:39<00:00,  1.48it/s, loss=1.02]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.36407584045453373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [05:40<00:00,  1.48it/s, loss=0.387] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.28590205368174026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [05:37<00:00,  1.49it/s, loss=0.164] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 0.2253346254203127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [05:40<00:00,  1.48it/s, loss=0.053] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 0.176599095753613\n",
            "Validation Accuracy: 0.8787\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128, 256, 256, 256], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688, 0.6451331296377127, 0.8694893059799215, 0.878717964706616]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [05:36<00:00,  1.49it/s, loss=1.93] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.3388535781685575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [05:37<00:00,  1.49it/s, loss=1.56]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.4747830579480328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [05:37<00:00,  1.49it/s, loss=1.42]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.4717651087924304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [05:37<00:00,  1.49it/s, loss=1.48]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 1.470814642678219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [05:38<00:00,  1.48it/s, loss=1.71]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 1.4680953384395614\n",
            "Validation Accuracy: 0.4120\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128, 256, 256, 256, 256], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688, 0.6451331296377127, 0.8694893059799215, 0.878717964706616, 0.4120471409864688]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=1.72]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.567781114958197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=1.29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.433726050939218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [11:14<00:00,  1.34s/it, loss=1.27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.3487888455865868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [11:14<00:00,  1.34s/it, loss=1.17] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 1.251213628457362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [11:16<00:00,  1.35s/it, loss=0.57] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 1.1524583887293993\n",
            "Validation Accuracy: 0.6388\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128, 256, 256, 256, 256, 512], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688, 0.6451331296377127, 0.8694893059799215, 0.878717964706616, 0.4120471409864688, 0.6387728378125584]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.992]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.2188468352257018\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [11:15<00:00,  1.34s/it, loss=0.806]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.7336343935880053\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.539]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.5262224125375311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.229]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 0.4504181856801548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [11:14<00:00,  1.34s/it, loss=0.0338]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 0.40821864440354455\n",
            "Validation Accuracy: 0.8692\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128, 256, 256, 256, 256, 512, 512], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688, 0.6451331296377127, 0.8694893059799215, 0.878717964706616, 0.4120471409864688, 0.6387728378125584, 0.8691775269688845]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.163] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 0.6194193438289175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.0344]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.3523119836451998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.509] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.2721680678071016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.0401]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 0.20883439047506963\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 502/502 [11:15<00:00,  1.35s/it, loss=0.0218] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 0.14877604184100826\n",
            "Validation Accuracy: 0.8749\n",
            "{'lr': [1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-07, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05, 0.0001, 1e-07, 1e-06, 1e-05], 'mlen': [128, 128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 128, 128, 128, 128, 256, 256, 256, 256, 512, 512, 512], 'batch_size': [8, 8, 8, 8, 16, 16, 16, 16, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 8, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32], 'accuracy': [0.7602419405125647, 0.8706740662218619, 0.8705493546174472, 0.4105506017334913, 0.6138928727318077, 0.8669327180894182, 0.8797780133441416, 0.4105506017334913, 0.7128515308349442, 0.7245744216499346, 0.8767225790359793, 0.875413107189624, 0.4079940138429881, 0.6717590571802706, 0.8716717590571803, 0.8793415227286899, 0.4079940138429881, 0.5928166115857081, 0.865623246243063, 0.7568747271933653, 0.7764544490864875, 0.8709858452328989, 0.8726070960902912, 0.4104258901290765, 0.617260086051007, 0.8694893059799215, 0.8764731558271497, 0.4120471409864688, 0.585084492111991, 0.8650620440231963, 0.8742283469476835, 0.4120471409864688, 0.6451331296377127, 0.8694893059799215, 0.878717964706616, 0.4120471409864688, 0.6387728378125584, 0.8691775269688845, 0.8749142607719649]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/502 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 31.50 GiB of which 172.12 MiB is free. Including non-PyTorch memory, this process has 31.33 GiB memory in use. Of the allocated memory 28.32 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     46\u001b[39m labels = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m loss = outputs.loss\n\u001b[32m     54\u001b[39m total_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1675\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1667\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1668\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1669\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1670\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1671\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1672\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1673\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1675\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1687\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1689\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:540\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m    539\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/transformers/activations.py:78\u001b[39m, in \u001b[36mGELUActivation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 31.50 GiB of which 172.12 MiB is free. Including non-PyTorch memory, this process has 31.33 GiB memory in use. Of the allocated memory 28.32 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# for p in params[11:]:\n",
        "# MAX_LEN = p[\"mlen\"]\n",
        "# BATCH_SIZE = p[\"batch_size\"]\n",
        "# EPOCHS = 5\n",
        "# LEARNING_RATE = p[\"lr\"]\n",
        "\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.000010\n",
        "\n",
        "model = \"bert-large-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "train_dataset = ClassificationDataset(train_text, train_label, tokenizer, MAX_LEN)\n",
        "test_dataset = ClassificationDataset(test_text, test_label, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Model and Device Setup\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    model, num_labels=grants[\"label_1\"].nunique()\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "\n",
        "    if (epoch > 0) and (epoch % 3 == 0):\n",
        "\n",
        "        time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "\n",
        "        model.push_to_hub(\n",
        "            repo_id, commit_message=f\"checkpoint_large_{time}_epoch{epoch}\"\n",
        "        )\n",
        "        tokenizer.push_to_hub(\n",
        "            repo_id,\n",
        "            commit_message=f\"checkpoint_large_{time}_epoch{epoch}\",\n",
        "        )\n",
        "\n",
        "    for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description(f\"Epoch {epoch}\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch} Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "model.push_to_hub(repo_id, commit_message=f\"trained_largebert_{time}_epoch{epoch}\")\n",
        "tokenizer.push_to_hub(\n",
        "    repo_id,\n",
        "    commit_message=f\"trained_largebert_{time}_epoch{epoch}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# hypers[\"lr\"].append(LEARNING_RATE)\n",
        "# hypers[\"mlen\"].append(MAX_LEN)\n",
        "# hypers[\"batch_size\"].append(BATCH_SIZE)\n",
        "# hypers[\"accuracy\"].append(accuracy)\n",
        "# print(hypers)\n",
        "# pd.DataFrame(hypers).to_parquet(workingdir + \"hyperparams_1digit_bigbert.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Look at fine tuning\n",
        "# df = pd.DataFrame(hypers).sort_values(\"accuracy\", ascending=False)\n",
        "# df.groupby(\"lr\")[\"accuracy\"].mean()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMaJO1nvot30P1L4VuMbE0H",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
