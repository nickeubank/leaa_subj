{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hKPY4yZo9neD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "# os.environ[\"PATH\"] = (\n",
        "#     \"/opt/apps/rhel9/cuda-12.4/bin:/opt/apps/rhel9/cuda-12.4/\" + os.environ[\"PATH\"]\n",
        "# )\n",
        "# os.environ[\"LD_LIBRARY_PATH\"] = (\n",
        "#     \"/opt/apps/rhel9/cuda-12.4/bin:/opt/apps/rhel9/cuda-12.4\"\n",
        "#     + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "# )\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (  # AdamW,\n",
        "    BertForSequenceClassification,\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "pd.set_option(\"mode.copy_on_write\", True)\n",
        "repo_id = \"nickeubank/leaa_grant_subjects_invweighted\"\n",
        "workingdir = \"/hpc/group/ssri/nce8/leaa_subj/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqAoIdGErrRL",
        "outputId": "da24dad9-3707-4a55-b04c-c567447deea1"
      },
      "outputs": [],
      "source": [
        "# dir = \"https://github.com/nickeubank/leaa_subj/raw/refs/heads/main/\"\n",
        "grants = pd.read_parquet(workingdir + \"subj_text_and_labels.parquet\")\n",
        "\n",
        "#########\n",
        "# Split into train test and for predict\n",
        "#########\n",
        "grants = grants.drop_duplicates(\"description\")\n",
        "\n",
        "labeled = grants[grants[\"label_1\"].notnull()]\n",
        "labeled[\"label_1_encoded\"] = labeled[\"label_1\"] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "proportions = labeled[\"label_1_encoded\"].value_counts(normalize=True).sort_index()\n",
        "inv_proportions = 1 / proportions\n",
        "inverse_weights = (inv_proportions / inv_proportions.sum()).astype(np.float32).values\n",
        "inverse_weights\n",
        "\n",
        "# Just make sure ordered right since that's critical\n",
        "assert max(inverse_weights) == inverse_weights[4]\n",
        "assert min(inverse_weights) == inverse_weights[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20046\n",
            "20047\n"
          ]
        }
      ],
      "source": [
        "labeled = labeled.sort_values(\"description\")\n",
        "\n",
        "labeled = labeled.sample(frac=0.5)  # CHANGE AFTER FINE TUNING\n",
        "\n",
        "train_label, test_label, train_text, test_text = train_test_split(\n",
        "    labeled[\"label_1_encoded\"].values,\n",
        "    labeled[\"description\"].values,\n",
        "    test_size=0.5,  # CHANGE AFTER FINE TUNING\n",
        "    random_state=45,\n",
        "    stratify=labeled[\"label_1_encoded\"],\n",
        ")\n",
        "print(len(train_label))\n",
        "print(len(test_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hpg0YddBshl9"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# Preprocess\n",
        "########\n",
        "\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hypertuning Parameters\n",
        "\n",
        "# # Run 1\n",
        "# params = [\n",
        "#     {\"mlen\": mlen, \"batch_size\": batch_size, \"lr\": lr}\n",
        "#     for batch_size in [8, 16, 32]\n",
        "#     for mlen in [128, 256, 512]\n",
        "#     for lr in [1e-7, 1e-6, 1e-5, 1e-4]\n",
        "# ]\n",
        "\n",
        "# df = pd.DataFrame(params)\n",
        "# df[\"accuracy\"] = np.nan\n",
        "# df[\"batch_size\"] = df[\"batch_size\"].astype(\"int\")\n",
        "# df[\"mlen\"] = df[\"mlen\"].astype(\"int\")\n",
        "# df.to_parquet(\"hyperparams_1digit_bigbert_invweights.parquet\")\n",
        "\n",
        "# Later Runs\n",
        "hypers = pd.read_parquet(\"hyperparams_1digit_bigbert_invweights.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, mlen          1.280000e+02\n",
            "batch_size    8.000000e+00\n",
            "lr            1.000000e-07\n",
            "accuracy               NaN\n",
            "Name: 0, dtype: float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipykernel_279734/1284542061.py:31: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1744247799952/work/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  weights = torch.from_numpy(inverse_weights)\n",
            "Epoch 0: 100%|██████████| 2506/2506 [06:54<00:00,  6.05it/s, loss=1.43]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.5895964227860582\n",
            "Validation Accuracy: 0.4234\n",
            "   mlen  batch_size            lr  accuracy\n",
            "0   128           8  1.000000e-07  0.423355\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 2506/2506 [06:53<00:00,  6.06it/s, loss=1.39] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.4508955907032002\n",
            "Validation Accuracy: 0.5574\n",
            "   mlen  batch_size            lr  accuracy\n",
            "0   128           8  1.000000e-07   0.55739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 2506/2506 [06:53<00:00,  6.07it/s, loss=1.24] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.2920053475443496\n",
            "Validation Accuracy: 0.7161\n",
            "   mlen  batch_size            lr  accuracy\n",
            "0   128           8  1.000000e-07  0.716117\n",
            "(1, mlen          128.000000\n",
            "batch_size      8.000000\n",
            "lr              0.000001\n",
            "accuracy             NaN\n",
            "Name: 1, dtype: float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 2506/2506 [06:52<00:00,  6.07it/s, loss=0.971]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.129651780026918\n",
            "Validation Accuracy: 0.8221\n",
            "   mlen  batch_size        lr  accuracy\n",
            "1   128           8  0.000001  0.822118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 2506/2506 [06:52<00:00,  6.08it/s, loss=1.11]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.5935281920205899\n",
            "Validation Accuracy: 0.8506\n",
            "   mlen  batch_size        lr  accuracy\n",
            "1   128           8  0.000001  0.850551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 2506/2506 [06:53<00:00,  6.06it/s, loss=0.307] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.5082022297580363\n",
            "Validation Accuracy: 0.8531\n",
            "   mlen  batch_size        lr  accuracy\n",
            "1   128           8  0.000001  0.853095\n",
            "(2, mlen          128.00000\n",
            "batch_size      8.00000\n",
            "lr              0.00001\n",
            "accuracy            NaN\n",
            "Name: 2, dtype: float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 2506/2506 [06:51<00:00,  6.09it/s, loss=0.795] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 0.6153049379853266\n",
            "Validation Accuracy: 0.8636\n",
            "   mlen  batch_size       lr  accuracy\n",
            "2   128           8  0.00001  0.863571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 2506/2506 [06:51<00:00,  6.09it/s, loss=0.0653] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.40511492495473356\n",
            "Validation Accuracy: 0.8797\n",
            "   mlen  batch_size       lr  accuracy\n",
            "2   128           8  0.00001  0.879733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 2506/2506 [06:52<00:00,  6.08it/s, loss=0.401]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.3058922572051385\n",
            "Validation Accuracy: 0.8657\n",
            "   mlen  batch_size       lr  accuracy\n",
            "2   128           8  0.00001  0.865716\n",
            "(3, mlen          128.0000\n",
            "batch_size      8.0000\n",
            "lr              0.0001\n",
            "accuracy           NaN\n",
            "Name: 3, dtype: float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 2506/2506 [06:51<00:00,  6.09it/s, loss=1.42]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.644203078718254\n",
            "Validation Accuracy: 0.4106\n",
            "   mlen  batch_size      lr  accuracy\n",
            "3   128           8  0.0001  0.410635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 2506/2506 [06:51<00:00,  6.09it/s, loss=1.6] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.6386883894348754\n",
            "Validation Accuracy: 0.4106\n",
            "   mlen  batch_size      lr  accuracy\n",
            "3   128           8  0.0001  0.410635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 2506/2506 [06:50<00:00,  6.10it/s, loss=1.7] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 1.6391388923762231\n",
            "Validation Accuracy: 0.1394\n",
            "   mlen  batch_size      lr  accuracy\n",
            "3   128           8  0.0001  0.139372\n",
            "(4, mlen          2.560000e+02\n",
            "batch_size    8.000000e+00\n",
            "lr            1.000000e-07\n",
            "accuracy               NaN\n",
            "Name: 4, dtype: float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 2506/2506 [10:04<00:00,  4.15it/s, loss=1.73]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 1.6444270133686751\n",
            "Validation Accuracy: 0.2728\n",
            "   mlen  batch_size            lr  accuracy\n",
            "4   256           8  1.000000e-07  0.272759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  93%|█████████▎| 2320/2506 [09:25<00:45,  4.10it/s, loss=1.24] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Use inverse weights and cross entropy\u001b[39;00m\n\u001b[32m     65\u001b[39m criterion = nn.CrossEntropyLoss(weight=weights)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m loss = criterion(outputs.get(\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m), labels)\n\u001b[32m     69\u001b[39m total_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:991\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffers.items():\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m         \u001b[38;5;28mself\u001b[39m._buffers[key] = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "for p in hypers[hypers[\"accuracy\"].isnull()].iterrows():\n",
        "    print(\"starting: \")\n",
        "    print(p)\n",
        "    MAX_LEN = int(p[1][\"mlen\"])\n",
        "    BATCH_SIZE = int(p[1][\"batch_size\"])\n",
        "    EPOCHS = 3\n",
        "    LEARNING_RATE = p[1][\"lr\"]\n",
        "\n",
        "    # MAX_LEN = 256\n",
        "    # BATCH_SIZE = 16\n",
        "    # EPOCHS = 10\n",
        "    # LEARNING_RATE = 0.000010\n",
        "\n",
        "    model = \"bert-large-uncased\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model)\n",
        "    train_dataset = ClassificationDataset(train_text, train_label, tokenizer, MAX_LEN)\n",
        "    test_dataset = ClassificationDataset(test_text, test_label, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Model and Device Setup\n",
        "    assert torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        model, num_labels=labeled[\"label_1_encoded\"].nunique()\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Need weights as tensors on gpu\n",
        "    weights = torch.from_numpy(inverse_weights)\n",
        "    weights.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Actual training\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        loop = tqdm(train_loader, leave=True)\n",
        "\n",
        "        # Checkpoints\n",
        "        if (epoch > 0) and (epoch % 3 == 0):\n",
        "\n",
        "            time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "\n",
        "            model.push_to_hub(\n",
        "                repo_id, commit_message=f\"checkpoint_large_{time}_epoch{epoch}\"\n",
        "            )\n",
        "\n",
        "        for batch in loop:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            # Use inverse weights and cross entropy\n",
        "            criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "            criterion.to(device)\n",
        "\n",
        "            loss = criterion(outputs.get(\"logits\"), labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loop.set_description(f\"Epoch {epoch}\")\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch} Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    ############\n",
        "    # Back to main flow\n",
        "    ############\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            formatted_to_list = list(map(lambda x: x.item(), predictions))\n",
        "            all_predictions.extend(formatted_to_list)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    hypers.loc[\n",
        "        (hypers[\"mlen\"] == MAX_LEN)\n",
        "        & (hypers[\"batch_size\"] == BATCH_SIZE)\n",
        "        & (hypers[\"lr\"] == LEARNING_RATE),\n",
        "        \"accuracy\",\n",
        "    ] = accuracy\n",
        "    print(\n",
        "        hypers[\n",
        "            (hypers[\"mlen\"] == MAX_LEN)\n",
        "            & (hypers[\"batch_size\"] == BATCH_SIZE)\n",
        "            & (hypers[\"lr\"] == LEARNING_RATE)\n",
        "        ]\n",
        "    )\n",
        "    pd.DataFrame(hypers).to_parquet(\n",
        "        workingdir + \"hyperparams_1digit_bigbert_invweights.parquet\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "########\n",
        "#\n",
        "# Test Sample Evaluation\n",
        "#\n",
        "#############\n",
        "# Load Model if not immediately after train\n",
        "############\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 256\n",
        "\n",
        "train_dataset = ClassificationDataset(train_text, train_label, tokenizer, MAX_LEN)\n",
        "test_dataset = ClassificationDataset(test_text, test_label, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(repo_id).to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>predicted</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>1904</td>\n",
              "      <td>89</td>\n",
              "      <td>114</td>\n",
              "      <td>79</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>118</td>\n",
              "      <td>6258</td>\n",
              "      <td>35</td>\n",
              "      <td>17</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>118</td>\n",
              "      <td>80</td>\n",
              "      <td>3151</td>\n",
              "      <td>91</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>116</td>\n",
              "      <td>52</td>\n",
              "      <td>154</td>\n",
              "      <td>2184</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>81</td>\n",
              "      <td>159</td>\n",
              "      <td>73</td>\n",
              "      <td>68</td>\n",
              "      <td>705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "predicted     0     1     2     3    4\n",
              "actual                                \n",
              "0.0        1904    89   114    79   48\n",
              "1.0         118  6258    35    17  149\n",
              "2.0         118    80  3151    91   95\n",
              "3.0         116    52   154  2184  100\n",
              "4.0          81   159    73    68  705"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_and_actual = pd.DataFrame(\n",
        "    {\"actual\": test_label, \"predicted\": all_predictions}\n",
        ")\n",
        "pd.crosstab(predictions_and_actual[\"actual\"], predictions_and_actual[\"predicted\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>predicted</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>11.9</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>0.7</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>19.6</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>4.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "predicted     0     1     2     3    4\n",
              "actual                                \n",
              "0.0        11.9   0.6   0.7   0.5  0.3\n",
              "1.0         0.7  39.0   0.2   0.1  0.9\n",
              "2.0         0.7   0.5  19.6   0.6  0.6\n",
              "3.0         0.7   0.3   1.0  13.6  0.6\n",
              "4.0         0.5   1.0   0.5   0.4  4.4"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion = pd.crosstab(\n",
        "    predictions_and_actual[\"actual\"],\n",
        "    predictions_and_actual[\"predicted\"],\n",
        "    normalize=True,\n",
        ")\n",
        "confusion.to_parquet(\"large_bert_confusion_matrix.parquet\")\n",
        "confusion_to_print = np.round(confusion * 100, decimals=1)\n",
        "confusion_to_print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>predicted</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>All</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>11.9</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>13.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>0.7</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.9</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>19.6</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.6</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>0.6</td>\n",
              "      <td>16.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>4.4</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>All</th>\n",
              "      <td>14.6</td>\n",
              "      <td>41.4</td>\n",
              "      <td>22.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>6.8</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "predicted     0     1     2     3    4    All\n",
              "actual                                       \n",
              "0.0        11.9   0.6   0.7   0.5  0.3   13.9\n",
              "1.0         0.7  39.0   0.2   0.1  0.9   41.0\n",
              "2.0         0.7   0.5  19.6   0.6  0.6   22.0\n",
              "3.0         0.7   0.3   1.0  13.6  0.6   16.2\n",
              "4.0         0.5   1.0   0.5   0.4  4.4    6.8\n",
              "All        14.6  41.4  22.0  15.2  6.8  100.0"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion = pd.crosstab(\n",
        "    predictions_and_actual[\"actual\"],\n",
        "    predictions_and_actual[\"predicted\"],\n",
        "    normalize=\"all\",\n",
        "    margins=True,\n",
        ")\n",
        "# confusion.to_parquet(\"large_bert_confusion_matrix_margins.parquet\")\n",
        "confusion_to_print = np.round(confusion * 100, decimals=1)\n",
        "confusion_to_print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mSignature:\u001b[39m\n",
            "pd.crosstab(\n",
            "    index,\n",
            "    columns,\n",
            "    values=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
            "    rownames=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
            "    colnames=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
            "    aggfunc=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
            "    margins: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
            "    margins_name: \u001b[33m'Hashable'\u001b[39m = \u001b[33m'All'\u001b[39m,\n",
            "    dropna: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
            "    normalize: \u001b[33m\"bool | Literal[0, 1, 'all', 'index', 'columns']\"\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
            ") -> \u001b[33m'DataFrame'\u001b[39m\n",
            "\u001b[31mDocstring:\u001b[39m\n",
            "Compute a simple cross tabulation of two (or more) factors.\n",
            "\n",
            "By default, computes a frequency table of the factors unless an\n",
            "array of values and an aggregation function are passed.\n",
            "\n",
            "Parameters\n",
            "----------\n",
            "index : array-like, Series, or list of arrays/Series\n",
            "    Values to group by in the rows.\n",
            "columns : array-like, Series, or list of arrays/Series\n",
            "    Values to group by in the columns.\n",
            "values : array-like, optional\n",
            "    Array of values to aggregate according to the factors.\n",
            "    Requires `aggfunc` be specified.\n",
            "rownames : sequence, default None\n",
            "    If passed, must match number of row arrays passed.\n",
            "colnames : sequence, default None\n",
            "    If passed, must match number of column arrays passed.\n",
            "aggfunc : function, optional\n",
            "    If specified, requires `values` be specified as well.\n",
            "margins : bool, default False\n",
            "    Add row/column margins (subtotals).\n",
            "margins_name : str, default 'All'\n",
            "    Name of the row/column that will contain the totals\n",
            "    when margins is True.\n",
            "dropna : bool, default True\n",
            "    Do not include columns whose entries are all NaN.\n",
            "normalize : bool, {'all', 'index', 'columns'}, or {0,1}, default False\n",
            "    Normalize by dividing all values by the sum of values.\n",
            "\n",
            "    - If passed 'all' or `True`, will normalize over all values.\n",
            "    - If passed 'index' will normalize over each row.\n",
            "    - If passed 'columns' will normalize over each column.\n",
            "    - If margins is `True`, will also normalize margin values.\n",
            "\n",
            "Returns\n",
            "-------\n",
            "DataFrame\n",
            "    Cross tabulation of the data.\n",
            "\n",
            "See Also\n",
            "--------\n",
            "DataFrame.pivot : Reshape data based on column values.\n",
            "pivot_table : Create a pivot table as a DataFrame.\n",
            "\n",
            "Notes\n",
            "-----\n",
            "Any Series passed will have their name attributes used unless row or column\n",
            "names for the cross-tabulation are specified.\n",
            "\n",
            "Any input passed containing Categorical data will have **all** of its\n",
            "categories included in the cross-tabulation, even if the actual data does\n",
            "not contain any instances of a particular category.\n",
            "\n",
            "In the event that there aren't overlapping indexes an empty DataFrame will\n",
            "be returned.\n",
            "\n",
            "Reference :ref:`the user guide <reshaping.crosstabulations>` for more examples.\n",
            "\n",
            "Examples\n",
            "--------\n",
            ">>> a = np.array([\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\",\n",
            "...               \"bar\", \"bar\", \"foo\", \"foo\", \"foo\"], dtype=object)\n",
            ">>> b = np.array([\"one\", \"one\", \"one\", \"two\", \"one\", \"one\",\n",
            "...               \"one\", \"two\", \"two\", \"two\", \"one\"], dtype=object)\n",
            ">>> c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\",\n",
            "...               \"shiny\", \"dull\", \"shiny\", \"shiny\", \"shiny\"],\n",
            "...              dtype=object)\n",
            ">>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])\n",
            "b   one        two\n",
            "c   dull shiny dull shiny\n",
            "a\n",
            "bar    1     2    1     0\n",
            "foo    2     2    1     2\n",
            "\n",
            "Here 'c' and 'f' are not represented in the data and will not be\n",
            "shown in the output because dropna is True by default. Set\n",
            "dropna=False to preserve categories with no data.\n",
            "\n",
            ">>> foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\n",
            ">>> bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])\n",
            ">>> pd.crosstab(foo, bar)\n",
            "col_0  d  e\n",
            "row_0\n",
            "a      1  0\n",
            "b      0  1\n",
            ">>> pd.crosstab(foo, bar, dropna=False)\n",
            "col_0  d  e  f\n",
            "row_0\n",
            "a      1  0  0\n",
            "b      0  1  0\n",
            "c      0  0  0\n",
            "\u001b[31mFile:\u001b[39m      /hpc/group/ssri/nce8/miniforge3/envs/torch/lib/python3.13/site-packages/pandas/core/reshape/pivot.py\n",
            "\u001b[31mType:\u001b[39m      function"
          ]
        }
      ],
      "source": [
        "pd.crosstab?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMaJO1nvot30P1L4VuMbE0H",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
